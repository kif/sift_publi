\documentclass[preprint]{iucr}
 \papertype{CP}
 \journalcode{S}



\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\begin{document}

%\title{Image registration and alignment algorithm for synchrotron applications}
\title{SIFT\_PyOCL : an implementation of SIFT in OpenCL}
\shorttitle{SIFT\_PyOCL}

    \author[a]{Pierre}{Paleo}
    \author[a]{Emeline}{Pouyet}
    \cauthor[a]{J\'er\^ome}{Kieffer}{jerome.kieffer@esrf,fr}{}
    \aff[a]{European Synchrotron Radiation Facility, \city{Grenoble}, \country{France}}
    \shortauthor{Paleo et al.}

\maketitle

\begin{synopsis}
An Python-OpenCL implementation of Scale-Invariant Feature Transform for image
alignment: application in Full-field X-Ray absorption spectroscopy.
\end{synopsis}

\begin{abstract}
The Scale Invariant Feature Transform (SIFT) algorithm allows image registration
and alignment. 
SIFT\_PyOCL is a Python module implementing a parallel version of this algoritm
in OpenCL,  providing high-speed image registration and alignment both on
processors and on graphics cards. 
The sub-pixel precision measured is suitable for full-field X-ray 
absorption spectroscopy stack alignement and the speed allows the processing of
such dataset online (i.e while experiment is going on).

\end{abstract}

\section{Introduction}

Image alignment is needed in many synchrotron beamlines for
various techniques like speckle images reconstruction in the field of coherent
X-ray diffraction imaging (CXDI) with module based pixel detectors or image
stack alignement for full-field X-ray absorption sppectroscopy (FFXAS).
To address this need we developped a parallel
version of the SIFT algorithm for image registration working both on multi-core
system and on graphics cards and interfaced in Python, a free scripting language
very popular among scientists. This high performance computing library complements
the toolbox based on NumPy \cite{numpy}, SciPy \cite{scipy} and
skimage \cite{skimage}.

\section{Full-Field X-ray absorption spectroscopy}
The European Synchrotron Radiation Facility (ESRF) beamline ID21 developed a
full-field method for X-ray absorption near-edge spectroscopy
(XANES) \cite{fullfield}. 
In this experiment, for each energy point across a given K- or L-edge,
a magnified 2D transmission image of the sample is acquired by a camera coupled
to an X-ray scintillator and magnifying visible light optics.
Then, a \emph{flat field} image, recorded without the sample, is used for
normalization. 
A XANES stack consists of a series of normalized images that characterizes the
sample absorption across the absorption edge of interest, each pixel being a
point in the XANES spectrum.
Since the \emph{flat field} images are not acquired simultaneously with the
sample transmission images, a realignment procedure has to be performed.


\section{Image alignemnt algorithm}

\subsection{The limits of phase correlation}

While phase correlation (in Fourier space) has intensively been used during the
development of FFXAS for stack alignment, this algorithm is limited to
translation and turned out to be very sensitive to artifacts, among those:
difference of intensity on  the image border, defects on the scintillator or on
the camera \ldots
Those defects could be corrected by some clever and sample
specific pre-processing like border cropping and apodisation. 
In order to make this procedure automatic and suppress human intervention, we
have considered image alignment based on keypoint extraction.

\subsection{Image registration algorithm}

The SIFT algorithm \cite{Lowe99,Lowe04}, initially developped
for image registration and widely used for panoramic image stitching has beed
adapted from the IPOL implementation \cite{ASIFT}.
Another registration algorithm, SURF (for Speeded Up Robust Feature
\cite{surf}), has been evaluated: it produces fewer keypoints with keypoint descriptor twice
smaller. 
While faster than SIFT and patent-free, this algorithm was not
retained due to it's shorter descriptor size (64 bytes instead of 128) making 
registration and matching less reliable.

Stack alignment obtained with SIFT in FFXAS were similar in quality to phase
correlation with a very good robustness to artifacts.
Moreover keypoints in some defective regions of the image could be
discarded easily using a mask.

Currently used at ESRF beamline ID21, the SIFT implementation takes about 8
seconds per 4 Mpixels frame  (when a stack can contain up to  500 frames) using
a single core.
Although the process is distributed using the EDNA \cite{edna} framework on a 
16-core computer, the performance limits are reached.
Image alignment being a crucial step in data pre-processing, the SIFT
algorithm had to be parallelized to take benefit from modern parallel
hardware like graphics cards (GPU) to obtain a faster data rate.
With the advent of high-performance GPU computing, many scientific data analysis
programs have already benefited from such parallelization
\cite{pyhst2,pyfai,pynx}.
%TODO :
% more

\subsection{SIFT algorithm overview}
%how it works
SIFT is an algorithm in computer-vision which can be used for image alignment
and pattern recognition.  

The keypoints are detected in several steps according to Lowe's
original paper\cite{Lowe99}.
%:\begin{itemize}
%%\setlength{\itemsep}{1pt}\setlength{\parskip}{0pt}\setlength{\parsep}{0pt}
%\item Keypoints detection: local extrema are detected in the \textit{scale-space} $(x, y, s)$. Every pixel is compared to its neighborhood in the image itself, and in the previous/next scale factor images.
%\item Keypoints refinement: keypoints located on corners are discarded. Additionaly, a second-order interpolation is done to improve the keypoints accuracy, modifying the coordinates $(x, y, s)$.
%\item Orientation assignment: a characteristic orientation is assigned to the keypoints $(x,y,s, \theta)$
%\item Descriptor computation: a histogram of orientations is built around every keypoint, then concatenated in a 128-values vector. This vector is called \textit{SIFT descriptor}, it is robust to rotation, illumination, translation and scaling.
%\end{itemize}
The scale variation is simulated by blurring the image.
A very blurred image represents a scene seen from a wider distance where
smaller details are not visible.


\subsubsection{Keypoints detection:}
The image is increasingly blurred, by convolution with gaussian kernels, to
imitate the scale variations.
Then, consecutives blurs are substracted to get the so-called differences of
gaussians (DoG) images where pixels are defined in a 3 dimensional
scale-space, composed by  their \emph{spacial} positions $(x,y)$ and their
\emph{scale} positions $s$ (related to the blur factor). Keypoints are
local maxima in this scale-space.
%The point :$(x,y,s)$ is a local maximum  if\begin{itemize}
%\setlength{\itemsep}{1pt}
%\setlength{\parskip}{0pt}
%\setlength{\parsep}{0pt}
%\item $D(x-1, y, s) < D(x,y,s)$ and $D(x,y,s) > D(x+1, y, s)$ (local maximum in $x$)
%\item $D(x, y-1, s) < D(x,y,s)$ and $D(x,y,s) > D(x, y+1, s)$ (local maximum in $y$)
%\item $D(x, y, s -1) < D(x,y,s)$ and $D(x,y,s) > D(x, y, s+1)$ (local maximum in $s$)
%\end{itemize}
%\pic{0.7}{img/dog1.png}{Detection in scale-space (source: en.wikipedia.org)}



\subsubsection{Keypoints refinement:}

At this stage, many keypoints are not reliable: low-contrast keypoints are
discarded, and keypoints located on an edge are rejected as well.  
%For keypoints located on an edge, principal curvature across the edge is much
%larger than the principal curvature along it. 
%The eigenvalues ratio $r$ of the Hessian matrix of the current DoG is compared
%to a threshold $\frac{(r+1)^2}{r} < R$. 

To improve keypoints accuracy, the coordinates are interpolated with a
second-order Taylor development. 
%\[
% D \left( \vec{x} + \vec{\delta_x} \right) \simeq D + \frac{\partial D}{\partial \vec{x}} \cdot \vec{\delta_x} + \frac{1}{2} \left( \vec{\delta_x} \right)^T \cdot \left( H \right) \cdot \vec{\delta_x} \qquad \text{with } H = \frac{\partial^2 D}{\partial \vec{x}^2}
%\]
% Keypoints that were too far from a true (interpolated) extremum are rejected. 


\subsubsection{Orientation assignment:}
The orientation of each keypoint is calculated  so that SIFT descriptors
can be invariant to rotation. 
For each blurred version of the image, the gradient magnitude and orientation
are computed. 
From the neighborhood of a keypoint, a histogram of orientations is built (36
bins, 1 bin per 10 degrees). 
%\pic{0.7}{img/orientation.png}{Orientation assignment}

The maximum value of this histogram is the dominant orientation; it is defined
as the characteristic orientation of the keypoint. 
Additionaly, every peak greater than 80\% of the maximum generates a new
keypoint with a different orientation. 



\subsubsection{Descriptor computation:}
A histogram of orientations is built around every keypoint. 
The neighborhood is divided into 4 regions of 4 subregions of 4x4 pixels. 
In every subregion, a 8-bin histogram is computed ; then, all the histograms are
concatenated in a 128-values descriptor. 
The histogram is weighted by the gradient magnitudes and the current scale
factor, so that the descriptor is robust to rotation, illumination, translation
and scaling.  

So a SIFT keypoint is composed of 4 floating point values: $x, y, scale$ and
$angle$, plus a descriptor of 128 8-bits integers representing the
orientation in the neighbourhood of the keypoint.

\section{Implementation}
While the phase correlation algorithm has been easily ported on graphics card
thanks to PyCUDA \cite{pyopencl} and cuFFT, hence very fast and well suited for
online data pre-processing;
the SIFT algorithm is much more complicated and the implementation available was
single threaded only.


\subsection{Parallelization of the algorithm}
SIFT\_PyOCL kernels were written in Open Computing Language \cite{opencl}
(OpenCL) so it can be run on various devices like GPU, multi-core CPU and
accelerators. 
They are launched from a Python module using PyOpenCL \cite{pyopencl},  which
provides both speed and code readability. 
The SIFT algorithm is used to align images with descriptors. 
Once descriptors of the two images are computed, a least-squares method is used
to determine the transformation aligning one image on the other.
%All the steps, from descriptor computation to images alignment, are done on the device to benefit from its parallelism.

Unlike existing parallel versions of SIFT\cite{lu,rister,vasilyev}, the entire
process is done on the device to avoid time-consuming transfers between CPU and
GPU. 
This leads to several subtle parts like the use of atomic instructions, or writing 
different versions of the same kernel to adapt to various platforms (CPU or
GPU) and compute capabilities of devices (for GPUs).

The first steps of the algorithm (keypoints detection and refinement) did not 
raise particular difficulty. 
For these steps, we highly benefit from the device parallelism : every pixel 
is handled by a single thread. 
Besides, convolution is implemented in the direct space (without Fourier
transform) and can be up to 50 times faster than the convolutions done by the
C++ reference implementation from IPOL.  
A pyramid is used to represent the image in scale-space \cite{Lowe04}.

The parallel implementation of the last steps (orientation assignment and 
descriptors computation) was more complex. 
For a given kernel, the performances strongly depend on the image
complexity and on the device the code is executed on; this is why different
versions have been written, each adapted for a different platforms and devices; 
the optimal version being selected by the
Python module based on compute capabilities of the selected device.


\subsection{Instalation and usage}
SIFT\_PyOCL can, as any Python module, be installed from its sources,
available on github \cite{sift_pyocl}.
While SIFT\_PyOCL is open source and licensed under a very
permissive BSD license; one should notice the SIFT algorithm itself is
patented by the Univerity of Columbia\cite{SIFT_pat}; neverthless this patent does
not apply in Europe.

Beside Python (version 2.6 or 2.7) and NumPy, SIFT\_PyOCL needs
PyOpenCL \cite{pyopencl}.
It was tested with the OpenCL drivers from Nvidia on a
large variete of their GPUs (Tesla, Fermi and Kepler generations) and on
multi-core processors with the driver from Intel and AMD. 
The full installation procedure with testing is simply:
\begin{verbatim}
$ python setup.py build test install
\end{verbatim}
Every single OpenCL kernel has been tested versus the reference
implementation; they can be run without installing the library by
executintg $test/test_all.py$. 
%(nota: not all kernels can run on all devices
%due to computing capabilities restrictions).

\subsection{Examples}

In this section we have collected some basic examples of how
SIFT\_PyOCL can be employed; using IPyhon \cite{ipython} in
pylab \cite{matplotlib} mode.

\subsubsection{Extract keypoints:}
\begin{verbatim}
In [1]: import fabio 
In [2]: img1 = fabio.open('image1.edf').data
In [3]: import sift
In [4]: siftplan = sift.SiftPlan(template=img1, devicetype="GPU")
In [5]: kp1 = siftplan.keypoints(img1)
\end{verbatim}

After having imported the FabIO \cite{fabio} module in [1], a first
absorption image is read in [2]. The SIFT\_PyOCL library is loaded in [3] and the
GPU is initialized in [4] with all memory allocated on the device (depending on
the image size).
In [4], the keypoint extraction took 60 ms for a
4 Mpixel image and returned a 261 keypoints vector as a numpy array. 
This figure varies a lot from sample to sample.

\subsubsection{Match keypoints between images:}
\begin{verbatim}
In [6]: img2 = fabio.open('image2.edf').data
In [7]: kp2 = siftplan.keypoints(img2)
In [8]: matchplan = sift.MatchPlan(devicetype="GPU")
In [9]: m = matchplan.match(kp1,kp2)
\end{verbatim}
Then a second image is read [6] and its keypoints are extracted [7].
In [8], a matching object is created, targeted to run on the graphic card.  
Keypoint association is done in [9], returning a numpy array of 66 lines and 2
columns of matched keypoints (execution time: 2.7 ms); each keypoint having
attributes $x$, $y$, $scale$ and $angle$

\subsubsection{Align an image on a reference}
\begin{verbatim}
In [10]: aligner = sift.LinearAlign(img1,devicetype='GPU')
In [11]: img2_cor = aligner.align(img2)
\end{verbatim}
It is also possible to align directly an image on a reference frame using an
afine transformation: combination of translation, rotation, homothety,
reflection \ldots  Just define a reference image and the kind on device [10];
the align method does keypoint extraction and matching, least-square optimization
of the afine transformation coeficients and finally applies the correction to
the frame[11], returning the corrected frame (execution time: 75ms, all timing
were measured on a Nvidia GeForce Titan as GPU).

\subsection{Performances}

The SIFT\_PyOCL has been benchmarked versus the IPOL reference implementation on
a dual Intel E5-2667 (12 cores, 2.90GHz) with a Nvidia Tesla K20m. The
acceleration measured on large images (more than 10 Mpixels) is between 30 times
and 50 times depending on the image complexity, on the GPU and from 4 times to
10 times when running on CPU.

%Shall we add a Table  Execution time/device ? 
\subsection{Limitations}
While all calculations are performed in single precision floating point which is
compatible even with the eldest graphic cards supported by OpenCL, the
memory consumption has been traded for performances. 
Moreover, the algorithm is suited
for linear color scales; making it unsuitable for high dynamic range
images like diffraction images.
Neverthless, using the difference of gaussian method like in
SIFT could be interesting for the extraction of spots for diffraction
images.


\section{Future and perspectives}

Registration of 3-dimentional object would have a huge application, especially
in the field of tomography and medical images; this field of reseach is very
active in applied mathematics and computer vision.


\section{Conclusion}

The SIFT algorithm is currently used at ESRF ID21 beamline for full field X-Ray
absorption spectroscopy image alignement thanks to it robustness to scale,
rotation and illumination changes. 
SIFT\_PyOCL has been implemented in parallel with adaptation to various
graphic cards and on multi-core processors where it shows interesing speed-ups.
Its programming interface tried to be simple to use and pythonic while doing
high performance computing under the hood. We believe it can be adopted by other
software developers to perform image alignement. This is why the code was made
open-source and free to re-distribute.


\ack{Acknowledgements}

The author would like to thank the ID21 team, especially the beamline
responsible, Marine Cotte, and Barbara Fayard, who is in charge of developing
the full field technique.
In the instrumentation division (ISDD) we would like to thank Claudio Ferrero,
head of data analysis, and Andy G\"otz, head of software, for supporting the
parallelization of this algorithm.

\bibliographystyle{iucr}
\bibliography{biblio}
%\referencelist[biblio]


\end{document}




